{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/mamba/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, [3]))\n",
    "# print('using GPU %s' % ','.join(map(str, [3])))\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from thop import profile, clever_format\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Times New Roman') \n",
    "\n",
    "from option import opt\n",
    "from loadData import data_pipe\n",
    "from loadData.dataAugmentation import dataAugmentation\n",
    "from models import cnns, vision_transformer, mamba, S2ENet, FusAtNet\n",
    "from models.MS2CANet import pymodel\n",
    "from models.CrossHL import CrossHL\n",
    "from models.HCTNet import HCTNet\n",
    "from models.DSHFNet import DSHF\n",
    "from models.MIViT import MMA\n",
    "\n",
    "from utils import trainer, tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.backbone DSHFNet\n"
     ]
    }
   ],
   "source": [
    "args = opt.get_args()\n",
    "# args.dataset_name = \"PaviaU\"\n",
    "args.dataset_name = \"Houston_2013\"\n",
    "\n",
    "\n",
    "args.backbone = \"vit\"   \n",
    "args.backbone = \"cnn\"   \n",
    "# args.backbone = \"mamba\" \n",
    "\n",
    "# args.backbone = \"MS2CANet\"\n",
    "# args.backbone = \"S2ENet\"    \n",
    "# args.backbone = \"FusAtNet\"  \n",
    "# args.backbone = \"CrossHL\"  \n",
    "# args.backbone = \"HCTNet\"\n",
    "# args.backbone = \"MIViT\"  \n",
    "args.backbone = \"DSHFNet\"\n",
    "\n",
    "args.pca = True\n",
    "\n",
    "if args.backbone == \"FusAtNet\":\n",
    "    args.epochs = 1000\n",
    "    args.patch_size = 11\n",
    "    args.components = 20\n",
    "    args.learning_rate = 0.000005\n",
    "\n",
    "elif  args.backbone == \"CrossHL\":\n",
    "    args.epochs = 200\n",
    "    args.patch_size = 11\n",
    "    args.batch_size = 64\n",
    "    args.pca = False\n",
    "    args.learning_rate = 0.0005\n",
    "\n",
    "elif args.backbone == \"MS2CANet\":\n",
    "    args.epochs = 100\n",
    "    args.patch_size = 11\n",
    "    args.batch_size = 64\n",
    "    args.learning_rate = 0.001\n",
    "    args.components = 20    # Houston\n",
    "\n",
    "elif args.backbone == \"S2ENet\":\n",
    "    args.epochs = 128\n",
    "    args.batch_size = 64\n",
    "    args.patch_size = 7\n",
    "    args.components = 20    # Houston\n",
    "    args.learning_rate = 0.001\n",
    "\n",
    "elif args.backbone == \"HCTNet\":\n",
    "    args.epochs = 100\n",
    "    args.batch_size = 64\n",
    "    args.patch_size = 11\n",
    "    args.components = 20    # Houston\n",
    "    args.learning_rate = 0.001\n",
    "    # args.components = 30    # Trento_train2\n",
    "    \n",
    "elif args.backbone == \"DSHFNet\":\n",
    "    args.epochs = 500\n",
    "    args.batch_size = 64\n",
    "    args.patch_size = 6\n",
    "    args.pca = False\n",
    "    args.learning_rate = 5e-4\n",
    "    args.weight_decay = 0\n",
    "    args.gamma = 0.9\n",
    "\n",
    "elif args.backbone == \"MIViT\":\n",
    "    args.epochs = 500\n",
    "    # args.epochs = 11\n",
    "    args.patch_size = 8\n",
    "    args.batch_size = 64\n",
    "    args.learning_rate = 1e-4\n",
    "    args.weight_decay = 0.001\n",
    "    args.gamma = 0.9\n",
    "    args.fusion = 'TTOA'\n",
    "    args.pred_flag = 'o_fuse'\n",
    "    \n",
    "else:\n",
    "    args.epochs = 100\n",
    "    args.batch_size = 64\n",
    "    args.patch_size = 11\n",
    "    # args.patch_size = 12\n",
    "    # args.randomCrop = 8\n",
    "    args.learning_rate = 0.001\n",
    "    args.components = 15    \n",
    "\n",
    "args.randomCrop = 11\n",
    "print(\"args.backbone\", args.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca is not used\n",
      "split_type:  disjoint train_ratio:  1\n",
      "split_type:  disjoint train_ratio:  1\n",
      "split_type:  disjoint train_ratio:  1\n",
      "print_data_info : ---->\n",
      "class 1 \t 198 \t 1053\n",
      "class 2 \t 190 \t 1064\n",
      "class 3 \t 192 \t 505\n",
      "class 4 \t 188 \t 1056\n",
      "class 5 \t 186 \t 1056\n",
      "class 6 \t 182 \t 143\n",
      "class 7 \t 196 \t 1072\n",
      "class 8 \t 191 \t 1053\n",
      "class 9 \t 193 \t 1059\n",
      "class 10 \t 191 \t 1036\n",
      "class 11 \t 181 \t 1054\n",
      "class 12 \t 192 \t 1041\n",
      "class 13 \t 184 \t 285\n",
      "class 14 \t 181 \t 247\n",
      "class 15 \t 187 \t 473\n",
      "total     \t 2832 \t 12197\n",
      "mutlisacle multimodality\n",
      "data1 (367, 1923, 144) data2 (367, 1923, 1)\n"
     ]
    }
   ],
   "source": [
    "# data_pipe.set_deterministic(seed = 666)\n",
    "args.print_data_info = True\n",
    "args.data_info_start = 1\n",
    "args.show_gt = False\n",
    "args.remove_zero_labels = True\n",
    "args.split_type = \"disjoint\"\n",
    "# transform = dataAugmentation(args.randomCrop)\n",
    "transform = None\n",
    "\n",
    "\n",
    "# create dataloader\n",
    "if args.dataset_name in args.SD:\n",
    "    args.train_ratio = 0.1\n",
    "    args.path_data = \"/home/leo/DatasetSMD\"\n",
    "    img1, train_gt, test_gt, data_gt = data_pipe.get_data(args)\n",
    "elif args.dataset_name in args.MD:\n",
    "    args.train_ratio = 1\n",
    "    args.path_data = \"/home/leo/DatasetMMF\"\n",
    "    img1, img2, train_gt, test_gt, data_gt = data_pipe.get_data(args)\n",
    "\n",
    "\n",
    "if args.backbone in args.MMISO or args.backbone in args.MMIMO:\n",
    "    print(\"mutlisacle multimodality\")\n",
    "    # 在这直接输出多尺度的图像\n",
    "    train_dataset = data_pipe.HyperXMM(img1, data2=img2, gt=train_gt, \n",
    "                                    transform=None, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    test_dataset = data_pipe.HyperXMM(img1, data2=img2, gt=test_gt, \n",
    "                                    transform=None, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    \n",
    "    height, wigth, data1_bands = train_dataset.data1.shape\n",
    "    height, wigth, data2_bands = train_dataset.data2.shape\n",
    "    print(\"data1\", train_dataset.data1.shape, \"data2\", train_dataset.data2.shape)\n",
    "\n",
    "\n",
    "# elif args.backbone in args.SSSM:\n",
    "#     print(\"singlescale singlemodality\")\n",
    "#     transform = dataAugmentation(args.randomCrop)\n",
    "#     train_dataset = data_pipe.HyperX(img1, gt=train_gt, transform=transform, patch_size=args.patch_size, \n",
    "#                             remove_zero_labels=args.remove_zero_labels)\n",
    "#     test_dataset = data_pipe.HyperX(img1, gt=test_gt, transform=None, patch_size=args.patch_size, \n",
    "#                             remove_zero_labels=args.remove_zero_labels)\n",
    "    \n",
    "#     height, wigth, data1_bands = train_dataset.data1.shape\n",
    "#     print(\"data1\", train_dataset.data1.shape)\n",
    "    \n",
    "\n",
    "elif args.backbone in args.SSISO or args.backbone in args.SMIMO or args.backbone in args.SMISO:\n",
    "    print(\"singlescale multimodality\")\n",
    "    train_dataset = data_pipe.HyperX(img1, data2=img2, gt=train_gt, \n",
    "                                    transform=transform, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    test_dataset = data_pipe.HyperX(img1, data2=img2, gt=test_gt, \n",
    "                                    transform=None, patch_size=args.patch_size, \n",
    "                                    remove_zero_labels=args.remove_zero_labels)\n",
    "    \n",
    "    height, wigth, data1_bands = train_dataset.data1.shape\n",
    "    height, wigth, data2_bands = train_dataset.data2.shape\n",
    "    print(\"data1\", train_dataset.data1.shape, \"data2\", train_dataset.data2.shape)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "class_num = np.max(train_gt)\n",
    "# print(class_num, train_gt.shape, len(train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape, y.shape torch.Size([64, 144, 6, 6]) torch.Size([64, 144, 12, 12]) torch.Size([64, 144, 18, 18])\n",
      "x.shape, y.shape torch.float32 torch.float32 torch.float32\n",
      "x.shape, y.shape torch.Size([64, 1, 6, 6]) torch.Size([64, 1, 12, 12]) torch.Size([64, 1, 18, 18])\n",
      "x.shape, y.shape torch.float32 torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "if args.backbone in args.MMISO or args.backbone in args.MMIMO:\n",
    "    for data11, data12, data13, data21, data22, data23, label in train_loader:\n",
    "        print(\"x.shape, y.shape\", data11.shape, data12.shape, data13.shape)\n",
    "        print(\"x.shape, y.shape\", data11.dtype, data12.dtype, data13.dtype)\n",
    "\n",
    "        print(\"x.shape, y.shape\", data21.shape, data22.shape, data23.shape)\n",
    "        print(\"x.shape, y.shape\", data21.dtype, data22.dtype, data23.dtype)\n",
    "        break\n",
    "elif args.backbone in args.SSISO or args.backbone in args.SMISO or args.backbone in args.SMIMO:\n",
    "    for data11, data12, label in train_loader:\n",
    "        print(\"x.shape, y.shape\", data11.shape, data12.shape, label.shape)\n",
    "        print(\"x.shape, y.shape\", data11.dtype, data12.dtype, label.dtype)\n",
    "        break\n",
    "# elif args.backbone in args.SSSM:\n",
    "#     for x, z in train_loader:\n",
    "#         print(\"x.shape, y.shape\", x.shape, z.shape)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print((data21[0, 0, :, :].shape))\n",
    "# print((data22[0, 0, 3:9, 3:9].shape))\n",
    "# print((data23[0, 0, 6:12, 6:12].shape))\n",
    "# print(data21.shape)\n",
    "\n",
    "# # data21[20, 0, :, :], data22[20, 0, 3:9, 3:9], data23[20, 0, 6:12, 6:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leo/Multimodal_Classification/MyMultiModal/result/03-25-17-51-DSHFNet\n"
     ]
    }
   ],
   "source": [
    "args.result_dir = os.path.join(\"/home/leo/Multimodal_Classification/MyMultiModal/result\",\n",
    "                    datetime.now().strftime(\"%m-%d-%H-%M-\") + args.backbone)\n",
    "print(args.result_dir)\n",
    "\n",
    "# 加载已有权重路径\n",
    "# args.result_dir = \"/home/liuquanwei/code/DMVL_joint_MNDIS/results_final/08-09-17-05-vit_D8\"\n",
    "if not os.path.exists(args.result_dir):\n",
    "    os.mkdir(args.result_dir)\n",
    "with open(args.result_dir + '/args.json', 'w') as fid:\n",
    "    json.dump(args.__dict__, fid, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone:  DSHFNet\n"
     ]
    }
   ],
   "source": [
    "if args.backbone == \"cnn\":\n",
    "    model = cnns.Model_base(args.components).to(args.device)\n",
    "    args.feature_dim = 512\n",
    "    # args.feature_dim = 2048\n",
    "    super_head = cnns.FDGC_head(args.feature_dim, class_num=class_num).to(args.device)\n",
    "    params = list(super_head.parameters())  + list(model.parameters())\n",
    "\n",
    "elif args.backbone == \"vit\":\n",
    "    model = vision_transformer.vit_hsi(args.components, args.randomCrop).to(args.device)\n",
    "    # encoder = vision_transformer.vit_small(args.components, args.randomCrop).to(args.device)\n",
    "    args.feature_dim = 126\n",
    "    super_head = cnns.FDGC_head(args.feature_dim, class_num=class_num).to(args.device)\n",
    "    params = list(super_head.parameters())  + list(model.parameters())\n",
    "\n",
    "elif args.backbone == \"mamba\":\n",
    "    model = mamba.Vim(\n",
    "        dim=64,  # Dimension of the transformer model\n",
    "        # heads=8,  # Number of attention heads\n",
    "        dt_rank=32,  # Rank of the dynamic routing matrix\n",
    "        dim_inner=64,  # Inner dimension of the transformer model\n",
    "        d_state=64,  # Dimension of the state vector\n",
    "        num_classes=10,  # Number of output classes\n",
    "        image_size=args.randomCrop,  # Size of the input image\n",
    "        patch_size=4,  # Size of each image patch\n",
    "        channels=args.components,  # Number of input channels\n",
    "        dropout=0.1,  # Dropout rate\n",
    "        depth=4,  # Depth of the transformer model\n",
    "    ).to(args.device)\n",
    "    args.feature_dim = 64\n",
    "    super_head = cnns.FDGC_head(args.feature_dim, class_num=class_num).to(args.device)\n",
    "    params = list(super_head.parameters())  + list(model.parameters())\n",
    "\n",
    "elif args.backbone == \"MS2CANet\":\n",
    "    FM = 64\n",
    "    para_tune = False\n",
    "    if args.dataset_name == \"Houston_2013\":\n",
    "        para_tune = True                # para_tune 这个参数对于 Houston 的提升有两个点！！\n",
    "    model = pymodel.pyCNN(FM=FM, NC=data1_bands, \\\n",
    "                            Classes=class_num, para_tune=para_tune).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == 'S2ENet':\n",
    "    model = S2ENet.S2ENet(data1_bands, data2_bands, class_num, \\\n",
    "                            patch_size=args.patch_size).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"FusAtNet\":\n",
    "    model = FusAtNet.FusAtNet(data1_bands, data2_bands, class_num).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"CrossHL\":\n",
    "    FM = 16\n",
    "    model = CrossHL.CrossHL_Transformer(FM, data1_bands, data2_bands, class_num, \\\n",
    "                                args.patch_size).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"HCTNet\":\n",
    "    model = HCTNet(in_channels=1, num_classes=class_num).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"DSHFNet\":\n",
    "    model = DSHF(l1=data1_bands, l2=data2_bands, \\\n",
    "                         num_classes=class_num, encoder_embed_dim=64).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "elif args.backbone == \"MIViT\":\n",
    "    model = MMA.MMA(l1=data1_bands, l2=data2_bands, patch_size=args.patch_size, \\\n",
    "                num_patches=64, num_classes=class_num,\n",
    "                encoder_embed_dim=64, decoder_embed_dim=32, en_depth=5, \\\n",
    "                en_heads=4, de_depth=5, de_heads=4, mlp_dim=8, dropout=0.1, \\\n",
    "                emb_dropout=0.1,fusion=args.fusion).to(args.device)\n",
    "    params = model.parameters()\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"No models\")\n",
    "print(\"backbone: \", args.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flops, params = profile(net, inputs=(torch.randn(1, \n",
    "#                                                 args.components, \n",
    "#                                                 args.patch_size, \n",
    "#                                                 args.patch_size).cuda(),))\n",
    "# flops, params = clever_format([flops, params])\n",
    "# print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "\n",
    "# # contra_head\n",
    "# flops, params = profile(contra_head, inputs=(torch.randn(1, args.feature_dim).cuda(),))\n",
    "# flops, params = clever_format([flops, params])\n",
    "# print('# Model Params: {} FLOPs: {}'.format(params, flops))\n",
    "\n",
    "# # super_head\n",
    "# flops, params = profile(super_head, inputs=(torch.randn(1, args.feature_dim).cuda(),))\n",
    "# flops, params = clever_format([flops, params])\n",
    "# print('# Model Params: {} FLOPs: {}'.format(params, flops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "if args.backbone == \"CrossHL\":\n",
    "\toptimizer = optim.Adam(params, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.9)\n",
    "\t\n",
    "elif args.backbone == \"S2ENet\":\n",
    "\toptimizer = optim.Adam(params, lr=args.learning_rate)\n",
    "\tscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, \\\n",
    "\t                                                eta_min=1e-5, last_epoch=-1)\n",
    "\t\n",
    "elif args.backbone == \"DSHFNet\" or args.backbone == \"MIViT\":\n",
    "\toptimizer = optim.Adam(params, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.epochs // 10, gamma=args.gamma)\n",
    "\n",
    "else:\n",
    "\t# optimizer = optim.Adam(params, lr=args.learning_rate, weight_decay=1e-6, amsgrad=True)\n",
    "\toptimizer = optim.Adam(params, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\t# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = args.epochs, \\\n",
    "\t#                                                 eta_min = 1e-5, last_epoch = -1)\n",
    "\t# scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=60, T_mult=3, \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#    eta_min=1e-5)\n",
    "\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练前加载权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.resume = os.path.join(args.result_dir, \"joint_oa_model.pth\")\n",
    "if args.resume != '':\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    model.load_state_dict(checkpoint['base'], strict=False)\n",
    "    epoch_start = checkpoint['epoch'] + 1\n",
    "    print('Loaded from: {}'.format(args.resume))\n",
    "else:\n",
    "    epoch_start = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: [0/500] Loss: 2.6147 TRA: 23.7641 TEA: 22.2186 TIME: 17.1800\n",
      "Train Epoch: [1/500] Loss: 2.5542 TRA: 46.6455 TEA: 40.6329 TIME: 15.7900\n",
      "Train Epoch: [2/500] Loss: 2.3607 TRA: 51.4477 TEA: 42.6908 TIME: 15.3600\n",
      "Train Epoch: [3/500] Loss: 2.2985 TRA: 53.8136 TEA: 42.6088 TIME: 15.4600\n",
      "Train Epoch: [4/500] Loss: 2.2943 TRA: 55.2260 TEA: 42.8466 TIME: 15.4400\n",
      "Train Epoch: [5/500] Loss: 2.3404 TRA: 60.5226 TEA: 51.6848 TIME: 15.5600\n",
      "Train Epoch: [6/500] Loss: 2.1852 TRA: 65.0777 TEA: 51.7504 TIME: 15.2000\n",
      "Train Epoch: [7/500] Loss: 2.1091 TRA: 66.5254 TEA: 57.5879 TIME: 15.2600\n",
      "Train Epoch: [8/500] Loss: 2.0768 TRA: 67.9732 TEA: 57.7437 TIME: 15.5600\n",
      "Train Epoch: [9/500] Loss: 2.1103 TRA: 70.1271 TEA: 64.5487 TIME: 15.4400\n",
      "Train Epoch: [10/500] Loss: 2.0711 TRA: 75.0353 TEA: 64.3519 TIME: 15.3900\n",
      "Train Epoch: [11/500] Loss: 2.0100 TRA: 82.3799 TEA: 69.7385 TIME: 15.3500\n",
      "Train Epoch: [12/500] Loss: 2.0311 TRA: 89.1949 TEA: 74.7889 TIME: 15.3400\n",
      "Train Epoch: [13/500] Loss: 1.9696 TRA: 92.4788 TEA: 81.4709 TIME: 15.3000\n",
      "Train Epoch: [14/500] Loss: 1.9427 TRA: 95.0212 TEA: 79.2818 TIME: 15.7300\n",
      "Train Epoch: [15/500] Loss: 1.8990 TRA: 96.2924 TEA: 70.9109 TIME: 15.4100\n",
      "Train Epoch: [16/500] Loss: 1.8627 TRA: 96.9633 TEA: 75.6497 TIME: 15.5600\n",
      "Train Epoch: [17/500] Loss: 1.8566 TRA: 97.0692 TEA: 73.9936 TIME: 15.7800\n",
      "Train Epoch: [18/500] Loss: 1.8685 TRA: 98.1285 TEA: 76.8878 TIME: 15.7700\n",
      "Train Epoch: [19/500] Loss: 1.8370 TRA: 98.7288 TEA: 76.5106 TIME: 15.5000\n",
      "Train Epoch: [20/500] Loss: 1.8338 TRA: 98.7641 TEA: 71.2798 TIME: 15.9600\n",
      "Train Epoch: [21/500] Loss: 1.8577 TRA: 98.8701 TEA: 82.7335 TIME: 15.7700\n",
      "Train Epoch: [22/500] Loss: 1.8305 TRA: 98.9054 TEA: 81.0117 TIME: 15.7700\n",
      "Train Epoch: [23/500] Loss: 1.8458 TRA: 98.7994 TEA: 74.1986 TIME: 16.0100\n",
      "Train Epoch: [24/500] Loss: 1.8289 TRA: 98.7288 TEA: 80.4214 TIME: 15.5000\n",
      "Train Epoch: [25/500] Loss: 1.8441 TRA: 99.1525 TEA: 78.5275 TIME: 15.8800\n",
      "Train Epoch: [26/500] Loss: 1.8314 TRA: 98.9054 TEA: 82.3317 TIME: 15.3500\n",
      "Train Epoch: [27/500] Loss: 1.8230 TRA: 99.0819 TEA: 80.1181 TIME: 15.5300\n",
      "Train Epoch: [28/500] Loss: 1.8353 TRA: 98.9407 TEA: 83.6845 TIME: 15.9700\n",
      "Train Epoch: [29/500] Loss: 1.8202 TRA: 98.8701 TEA: 81.1921 TIME: 15.8900\n",
      "Train Epoch: [30/500] Loss: 1.8260 TRA: 99.3997 TEA: 82.8646 TIME: 15.7300\n",
      "Train Epoch: [31/500] Loss: 1.8217 TRA: 99.3997 TEA: 81.7906 TIME: 15.5800\n",
      "Train Epoch: [32/500] Loss: 1.8199 TRA: 99.2938 TEA: 82.6761 TIME: 15.8800\n",
      "Train Epoch: [33/500] Loss: 1.8282 TRA: 99.2585 TEA: 85.2587 TIME: 16.0300\n",
      "Train Epoch: [34/500] Loss: 1.8219 TRA: 99.0819 TEA: 84.7995 TIME: 16.1100\n",
      "Train Epoch: [35/500] Loss: 1.8215 TRA: 99.2232 TEA: 84.5618 TIME: 15.6900\n",
      "Train Epoch: [36/500] Loss: 1.8203 TRA: 99.3291 TEA: 83.6517 TIME: 15.6700\n",
      "Train Epoch: [37/500] Loss: 1.8279 TRA: 99.3644 TEA: 85.8654 TIME: 16.5400\n",
      "Train Epoch: [38/500] Loss: 1.8189 TRA: 99.2585 TEA: 83.2910 TIME: 16.3200\n",
      "Train Epoch: [39/500] Loss: 1.8233 TRA: 99.0466 TEA: 81.5282 TIME: 16.2200\n",
      "Train Epoch: [40/500] Loss: 1.8207 TRA: 99.1172 TEA: 85.9884 TIME: 15.7100\n",
      "Train Epoch: [41/500] Loss: 1.8215 TRA: 99.3644 TEA: 87.4805 TIME: 15.8800\n",
      "Train Epoch: [42/500] Loss: 1.8226 TRA: 99.3291 TEA: 85.8080 TIME: 15.9200\n",
      "Train Epoch: [43/500] Loss: 1.8183 TRA: 99.3644 TEA: 80.7658 TIME: 15.8100\n",
      "Train Epoch: [44/500] Loss: 1.8194 TRA: 99.3291 TEA: 82.4137 TIME: 15.8200\n",
      "Train Epoch: [45/500] Loss: 1.8216 TRA: 99.3291 TEA: 83.7091 TIME: 15.6300\n",
      "Train Epoch: [46/500] Loss: 1.8177 TRA: 99.4350 TEA: 84.9963 TIME: 15.7800\n",
      "Train Epoch: [47/500] Loss: 1.8174 TRA: 99.4350 TEA: 85.2423 TIME: 16.0300\n",
      "Train Epoch: [48/500] Loss: 1.8176 TRA: 99.4350 TEA: 85.2423 TIME: 15.9900\n",
      "Train Epoch: [49/500] Loss: 1.8178 TRA: 99.4350 TEA: 85.1685 TIME: 16.0700\n",
      "Train Epoch: [50/500] Loss: 1.8176 TRA: 99.2938 TEA: 84.3240 TIME: 15.8000\n",
      "Train Epoch: [51/500] Loss: 1.8175 TRA: 99.3644 TEA: 85.7916 TIME: 15.7500\n",
      "Train Epoch: [52/500] Loss: 1.8191 TRA: 99.3644 TEA: 87.7921 TIME: 16.1800\n",
      "Train Epoch: [53/500] Loss: 1.8177 TRA: 99.4350 TEA: 84.2994 TIME: 16.3200\n",
      "Train Epoch: [54/500] Loss: 1.8261 TRA: 99.3644 TEA: 85.5702 TIME: 15.9600\n",
      "Train Epoch: [55/500] Loss: 1.8177 TRA: 99.4350 TEA: 85.7014 TIME: 16.4700\n",
      "Train Epoch: [56/500] Loss: 1.8295 TRA: 99.3291 TEA: 85.3735 TIME: 16.6400\n",
      "Train Epoch: [57/500] Loss: 1.8346 TRA: 99.2938 TEA: 84.9389 TIME: 15.6100\n",
      "Train Epoch: [58/500] Loss: 1.8177 TRA: 99.3291 TEA: 85.8162 TIME: 15.5300\n",
      "Train Epoch: [59/500] Loss: 1.8184 TRA: 99.3997 TEA: 86.6771 TIME: 16.1200\n",
      "Train Epoch: [60/500] Loss: 1.8172 TRA: 99.3997 TEA: 83.7419 TIME: 15.5400\n",
      "Train Epoch: [61/500] Loss: 1.8171 TRA: 99.4350 TEA: 85.0619 TIME: 15.2900\n",
      "Train Epoch: [62/500] Loss: 1.8169 TRA: 99.4350 TEA: 84.3322 TIME: 15.3100\n",
      "Train Epoch: [63/500] Loss: 1.8199 TRA: 99.4350 TEA: 86.2753 TIME: 15.6400\n",
      "Train Epoch: [64/500] Loss: 1.8172 TRA: 99.4350 TEA: 85.8982 TIME: 16.1000\n",
      "Train Epoch: [65/500] Loss: 1.8170 TRA: 99.4350 TEA: 86.2015 TIME: 16.0500\n",
      "Train Epoch: [66/500] Loss: 1.8170 TRA: 99.4350 TEA: 86.0539 TIME: 15.9100\n",
      "Train Epoch: [67/500] Loss: 1.8195 TRA: 99.3291 TEA: 85.3407 TIME: 16.0800\n",
      "Train Epoch: [68/500] Loss: 1.8185 TRA: 99.2232 TEA: 83.2500 TIME: 16.0100\n",
      "Train Epoch: [69/500] Loss: 1.8181 TRA: 99.2232 TEA: 85.1685 TIME: 15.5100\n",
      "Train Epoch: [70/500] Loss: 1.8203 TRA: 99.2585 TEA: 84.6930 TIME: 15.4600\n",
      "Train Epoch: [71/500] Loss: 1.8175 TRA: 99.4350 TEA: 86.3655 TIME: 15.5300\n",
      "Train Epoch: [72/500] Loss: 1.8170 TRA: 99.4350 TEA: 84.6602 TIME: 15.9000\n",
      "Train Epoch: [73/500] Loss: 1.8195 TRA: 99.3291 TEA: 83.7009 TIME: 15.6100\n",
      "Train Epoch: [74/500] Loss: 1.8171 TRA: 99.4350 TEA: 83.6517 TIME: 15.3800\n",
      "Train Epoch: [75/500] Loss: 1.8172 TRA: 99.4350 TEA: 85.3735 TIME: 15.2800\n",
      "Train Epoch: [76/500] Loss: 1.8173 TRA: 99.4350 TEA: 84.9963 TIME: 15.5800\n",
      "Train Epoch: [77/500] Loss: 1.8169 TRA: 99.3997 TEA: 83.3976 TIME: 15.5700\n",
      "Train Epoch: [78/500] Loss: 1.8171 TRA: 99.3291 TEA: 83.3484 TIME: 15.6200\n",
      "Train Epoch: [79/500] Loss: 1.8170 TRA: 99.3997 TEA: 85.4718 TIME: 15.5400\n",
      "Train Epoch: [80/500] Loss: 1.8181 TRA: 99.4350 TEA: 84.9225 TIME: 15.4200\n",
      "Train Epoch: [81/500] Loss: 1.8341 TRA: 99.1879 TEA: 78.0848 TIME: 15.4000\n",
      "Train Epoch: [82/500] Loss: 1.8176 TRA: 99.0466 TEA: 83.8895 TIME: 15.4000\n",
      "Train Epoch: [83/500] Loss: 1.8256 TRA: 99.2938 TEA: 83.7419 TIME: 15.4600\n",
      "Train Epoch: [84/500] Loss: 1.8169 TRA: 99.2938 TEA: 84.7831 TIME: 15.6600\n",
      "Train Epoch: [85/500] Loss: 1.8175 TRA: 99.3644 TEA: 80.4706 TIME: 15.5200\n",
      "Train Epoch: [86/500] Loss: 1.8171 TRA: 99.1172 TEA: 83.6681 TIME: 15.4700\n",
      "Train Epoch: [87/500] Loss: 1.8169 TRA: 99.4350 TEA: 85.1275 TIME: 15.3800\n",
      "Train Epoch: [88/500] Loss: 1.8170 TRA: 99.4350 TEA: 86.0949 TIME: 15.1300\n",
      "Train Epoch: [89/500] Loss: 1.8175 TRA: 99.3997 TEA: 86.2425 TIME: 15.6000\n",
      "Train Epoch: [90/500] Loss: 1.8169 TRA: 99.4350 TEA: 85.8818 TIME: 15.5100\n",
      "Train Epoch: [91/500] Loss: 1.8174 TRA: 99.3644 TEA: 85.6440 TIME: 15.6000\n",
      "Train Epoch: [92/500] Loss: 1.8169 TRA: 99.3997 TEA: 86.3491 TIME: 15.5400\n",
      "Train Epoch: [93/500] Loss: 1.8169 TRA: 99.3997 TEA: 85.8162 TIME: 15.4500\n",
      "Train Epoch: [94/500] Loss: 1.8167 TRA: 99.3997 TEA: 86.8000 TIME: 15.2800\n",
      "Train Epoch: [95/500] Loss: 1.8167 TRA: 99.4350 TEA: 85.0701 TIME: 15.3600\n",
      "Train Epoch: [96/500] Loss: 1.8167 TRA: 99.4350 TEA: 83.2008 TIME: 15.3000\n",
      "Train Epoch: [97/500] Loss: 1.8181 TRA: 99.3997 TEA: 84.8651 TIME: 15.4300\n",
      "Train Epoch: [98/500] Loss: 1.8170 TRA: 99.3997 TEA: 82.6187 TIME: 15.3400\n",
      "Train Epoch: [99/500] Loss: 1.8166 TRA: 99.4350 TEA: 83.6353 TIME: 15.2400\n",
      "Train Epoch: [100/500] Loss: 1.8168 TRA: 99.4350 TEA: 84.2010 TIME: 15.2800\n",
      "Train Epoch: [101/500] Loss: 1.8167 TRA: 99.4350 TEA: 85.4062 TIME: 15.4100\n",
      "Train Epoch: [102/500] Loss: 1.8167 TRA: 99.3997 TEA: 84.9225 TIME: 15.3600\n",
      "Train Epoch: [103/500] Loss: 1.8168 TRA: 99.3291 TEA: 81.0609 TIME: 15.6800\n",
      "Train Epoch: [104/500] Loss: 1.8179 TRA: 99.3997 TEA: 84.5454 TIME: 15.7000\n",
      "Train Epoch: [105/500] Loss: 1.8190 TRA: 99.3644 TEA: 83.7583 TIME: 15.3400\n",
      "Train Epoch: [106/500] Loss: 1.8167 TRA: 99.4350 TEA: 84.2010 TIME: 15.6200\n",
      "Train Epoch: [107/500] Loss: 1.8173 TRA: 99.2232 TEA: 83.0368 TIME: 15.5000\n",
      "Train Epoch: [108/500] Loss: 1.8171 TRA: 98.7641 TEA: 82.3645 TIME: 15.5900\n",
      "Train Epoch: [109/500] Loss: 1.8167 TRA: 99.3997 TEA: 87.8331 TIME: 15.4100\n",
      "Train Epoch: [110/500] Loss: 1.8170 TRA: 99.3997 TEA: 85.9392 TIME: 15.7200\n",
      "Train Epoch: [111/500] Loss: 1.8169 TRA: 99.3997 TEA: 86.5295 TIME: 15.8000\n",
      "Train Epoch: [112/500] Loss: 1.8183 TRA: 99.4350 TEA: 87.3493 TIME: 15.5600\n",
      "Train Epoch: [113/500] Loss: 1.8166 TRA: 99.3997 TEA: 84.9225 TIME: 15.7900\n",
      "Train Epoch: [114/500] Loss: 1.8167 TRA: 99.3644 TEA: 86.5623 TIME: 15.4300\n",
      "Train Epoch: [115/500] Loss: 1.8167 TRA: 99.4350 TEA: 86.6689 TIME: 15.4700\n",
      "Train Epoch: [116/500] Loss: 1.8166 TRA: 99.4350 TEA: 84.9635 TIME: 15.6400\n",
      "Train Epoch: [117/500] Loss: 1.8167 TRA: 99.4350 TEA: 86.6525 TIME: 15.1800\n",
      "Train Epoch: [118/500] Loss: 1.8168 TRA: 99.4350 TEA: 86.0376 TIME: 15.3000\n",
      "Train Epoch: [119/500] Loss: 1.8167 TRA: 99.4350 TEA: 85.7178 TIME: 15.5400\n",
      "Train Epoch: [120/500] Loss: 1.8190 TRA: 99.4350 TEA: 85.0291 TIME: 15.4300\n",
      "Train Epoch: [121/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.3163 TIME: 15.4200\n",
      "Train Epoch: [122/500] Loss: 1.8236 TRA: 99.3644 TEA: 85.3898 TIME: 15.2300\n",
      "Train Epoch: [123/500] Loss: 1.8167 TRA: 99.3644 TEA: 84.8241 TIME: 15.2400\n",
      "Train Epoch: [124/500] Loss: 1.8169 TRA: 99.2585 TEA: 80.0033 TIME: 15.2900\n",
      "Train Epoch: [125/500] Loss: 1.8166 TRA: 99.4350 TEA: 84.5372 TIME: 15.2300\n",
      "Train Epoch: [126/500] Loss: 1.8178 TRA: 99.3997 TEA: 84.3240 TIME: 15.3000\n",
      "Train Epoch: [127/500] Loss: 1.8166 TRA: 99.3997 TEA: 84.9799 TIME: 15.2400\n",
      "Train Epoch: [128/500] Loss: 1.8167 TRA: 99.4350 TEA: 83.9961 TIME: 15.3000\n",
      "Train Epoch: [129/500] Loss: 1.8169 TRA: 99.4350 TEA: 86.0130 TIME: 15.3000\n",
      "Train Epoch: [130/500] Loss: 1.8167 TRA: 99.3644 TEA: 77.2813 TIME: 15.3300\n",
      "Train Epoch: [131/500] Loss: 1.8320 TRA: 98.8701 TEA: 70.8863 TIME: 15.3400\n",
      "Train Epoch: [132/500] Loss: 1.8214 TRA: 99.1879 TEA: 85.6194 TIME: 15.4600\n",
      "Train Epoch: [133/500] Loss: 1.8167 TRA: 99.3644 TEA: 85.7096 TIME: 15.1600\n",
      "Train Epoch: [134/500] Loss: 1.8170 TRA: 99.3997 TEA: 86.6033 TIME: 15.1300\n",
      "Train Epoch: [135/500] Loss: 1.8171 TRA: 99.3644 TEA: 86.5787 TIME: 15.4800\n",
      "Train Epoch: [136/500] Loss: 1.8168 TRA: 99.3644 TEA: 85.4800 TIME: 15.0000\n",
      "Train Epoch: [137/500] Loss: 1.8166 TRA: 99.4350 TEA: 85.1029 TIME: 15.3300\n",
      "Train Epoch: [138/500] Loss: 1.8167 TRA: 99.4350 TEA: 85.7096 TIME: 15.4500\n",
      "Train Epoch: [139/500] Loss: 1.8167 TRA: 99.4350 TEA: 86.1113 TIME: 15.1200\n",
      "Train Epoch: [140/500] Loss: 1.8166 TRA: 99.2938 TEA: 86.9804 TIME: 15.2200\n",
      "Train Epoch: [141/500] Loss: 1.8169 TRA: 99.4350 TEA: 85.4472 TIME: 15.0400\n",
      "Train Epoch: [142/500] Loss: 1.8166 TRA: 99.4350 TEA: 84.9963 TIME: 15.4700\n",
      "Train Epoch: [143/500] Loss: 1.8166 TRA: 99.4350 TEA: 85.4226 TIME: 15.7300\n",
      "Train Epoch: [144/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.4231 TIME: 15.7300\n",
      "Train Epoch: [145/500] Loss: 1.8170 TRA: 99.2938 TEA: 84.4224 TIME: 14.9700\n",
      "Train Epoch: [146/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.5625 TIME: 15.1000\n",
      "Train Epoch: [147/500] Loss: 1.8168 TRA: 99.4350 TEA: 87.3739 TIME: 15.2600\n",
      "Train Epoch: [148/500] Loss: 1.8166 TRA: 99.4350 TEA: 73.3213 TIME: 15.2100\n",
      "Train Epoch: [149/500] Loss: 1.8184 TRA: 99.4350 TEA: 84.8077 TIME: 15.2100\n",
      "Train Epoch: [150/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.5541 TIME: 15.2100\n",
      "Train Epoch: [151/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.2261 TIME: 15.1400\n",
      "Train Epoch: [152/500] Loss: 1.8239 TRA: 99.4350 TEA: 85.7096 TIME: 15.4400\n",
      "Train Epoch: [153/500] Loss: 1.8165 TRA: 99.3997 TEA: 82.9138 TIME: 15.2400\n",
      "Train Epoch: [154/500] Loss: 1.8167 TRA: 99.3997 TEA: 85.0209 TIME: 15.1100\n",
      "Train Epoch: [155/500] Loss: 1.8175 TRA: 99.4350 TEA: 86.0130 TIME: 15.0800\n",
      "Train Epoch: [156/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.8162 TIME: 15.4400\n",
      "Train Epoch: [157/500] Loss: 1.8167 TRA: 99.3644 TEA: 84.2666 TIME: 15.0800\n",
      "Train Epoch: [158/500] Loss: 1.8166 TRA: 99.3644 TEA: 84.1272 TIME: 15.1800\n",
      "Train Epoch: [159/500] Loss: 1.8172 TRA: 99.3291 TEA: 85.1111 TIME: 15.2500\n",
      "Train Epoch: [160/500] Loss: 1.8166 TRA: 99.3644 TEA: 85.1357 TIME: 15.2200\n",
      "Train Epoch: [161/500] Loss: 1.8168 TRA: 99.3997 TEA: 88.5628 TIME: 15.3100\n",
      "Train Epoch: [162/500] Loss: 1.8169 TRA: 99.4350 TEA: 85.8080 TIME: 15.8000\n",
      "Train Epoch: [163/500] Loss: 1.8166 TRA: 99.4350 TEA: 85.6604 TIME: 15.6200\n",
      "Train Epoch: [164/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.1687 TIME: 15.7300\n",
      "Train Epoch: [165/500] Loss: 1.8166 TRA: 99.4350 TEA: 88.0298 TIME: 15.8400\n",
      "Train Epoch: [166/500] Loss: 1.8169 TRA: 99.4350 TEA: 86.3655 TIME: 15.7500\n",
      "Train Epoch: [167/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.0542 TIME: 15.5600\n",
      "Train Epoch: [168/500] Loss: 1.8166 TRA: 99.3997 TEA: 87.8167 TIME: 15.8000\n",
      "Train Epoch: [169/500] Loss: 1.8166 TRA: 99.4350 TEA: 88.1282 TIME: 15.7000\n",
      "Train Epoch: [170/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.4477 TIME: 15.6600\n",
      "Train Epoch: [171/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.8820 TIME: 15.6600\n",
      "Train Epoch: [172/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0050 TIME: 15.7800\n",
      "Train Epoch: [173/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.5951 TIME: 15.7900\n",
      "Train Epoch: [174/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.3163 TIME: 15.8900\n",
      "Train Epoch: [175/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.8326 TIME: 15.7800\n",
      "Train Epoch: [176/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8328 TIME: 15.8800\n",
      "Train Epoch: [177/500] Loss: 1.8172 TRA: 99.4350 TEA: 86.4229 TIME: 15.2000\n",
      "Train Epoch: [178/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.2756 TIME: 16.0100\n",
      "Train Epoch: [179/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.5625 TIME: 15.4700\n",
      "Train Epoch: [180/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.9233 TIME: 15.3700\n",
      "Train Epoch: [181/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.3493 TIME: 14.8400\n",
      "Train Epoch: [182/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6934 TIME: 15.2400\n",
      "Train Epoch: [183/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.4803 TIME: 15.3300\n",
      "Train Epoch: [184/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.7180 TIME: 15.0400\n",
      "Train Epoch: [185/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.1116 TIME: 15.1300\n",
      "Train Epoch: [186/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0870 TIME: 15.3200\n",
      "Train Epoch: [187/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9968 TIME: 15.2800\n",
      "Train Epoch: [188/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.3901 TIME: 15.2700\n",
      "Train Epoch: [189/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.2756 TIME: 15.2900\n",
      "Train Epoch: [190/500] Loss: 1.8167 TRA: 99.4350 TEA: 86.7262 TIME: 15.1400\n",
      "Train Epoch: [191/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.2346 TIME: 15.2600\n",
      "Train Epoch: [192/500] Loss: 1.8168 TRA: 99.4350 TEA: 86.5951 TIME: 15.3200\n",
      "Train Epoch: [193/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.6853 TIME: 15.5000\n",
      "Train Epoch: [194/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.3573 TIME: 14.9800\n",
      "Train Epoch: [195/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.4398 TIME: 15.7400\n",
      "Train Epoch: [196/500] Loss: 1.8169 TRA: 99.4350 TEA: 88.6939 TIME: 15.9900\n",
      "Train Epoch: [197/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.1362 TIME: 15.3500\n",
      "Train Epoch: [198/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7918 TIME: 15.3600\n",
      "Train Epoch: [199/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0050 TIME: 15.2700\n",
      "Train Epoch: [200/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6443 TIME: 15.2700\n",
      "Train Epoch: [201/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7262 TIME: 16.2300\n",
      "Train Epoch: [202/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9804 TIME: 14.8600\n",
      "Train Epoch: [203/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9230 TIME: 15.2000\n",
      "Train Epoch: [204/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.5871 TIME: 16.4800\n",
      "Train Epoch: [205/500] Loss: 1.8166 TRA: 99.3997 TEA: 86.6689 TIME: 16.1200\n",
      "Train Epoch: [206/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.9474 TIME: 16.2500\n",
      "Train Epoch: [207/500] Loss: 1.8166 TRA: 99.4350 TEA: 85.5046 TIME: 16.5100\n",
      "Train Epoch: [208/500] Loss: 1.8165 TRA: 99.3997 TEA: 68.3201 TIME: 16.1900\n",
      "Train Epoch: [209/500] Loss: 1.8180 TRA: 99.1172 TEA: 82.0612 TIME: 15.0400\n",
      "Train Epoch: [210/500] Loss: 1.8166 TRA: 99.4350 TEA: 84.6684 TIME: 15.2000\n",
      "Train Epoch: [211/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0952 TIME: 14.9500\n",
      "Train Epoch: [212/500] Loss: 1.8165 TRA: 99.3997 TEA: 85.4800 TIME: 15.7400\n",
      "Train Epoch: [213/500] Loss: 1.8166 TRA: 99.4350 TEA: 85.3079 TIME: 16.1800\n",
      "Train Epoch: [214/500] Loss: 1.8165 TRA: 99.3997 TEA: 85.2669 TIME: 15.0800\n",
      "Train Epoch: [215/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.3573 TIME: 15.0200\n",
      "Train Epoch: [216/500] Loss: 1.8167 TRA: 99.4350 TEA: 85.7670 TIME: 15.3000\n",
      "Train Epoch: [217/500] Loss: 1.8569 TRA: 99.0819 TEA: 79.9131 TIME: 15.2400\n",
      "Train Epoch: [218/500] Loss: 1.8168 TRA: 99.2232 TEA: 83.8075 TIME: 15.3200\n",
      "Train Epoch: [219/500] Loss: 1.8166 TRA: 99.3997 TEA: 87.9315 TIME: 15.7300\n",
      "Train Epoch: [220/500] Loss: 1.8166 TRA: 99.2938 TEA: 87.3166 TIME: 15.0400\n",
      "Train Epoch: [221/500] Loss: 1.8166 TRA: 99.3997 TEA: 86.8820 TIME: 16.2000\n",
      "Train Epoch: [222/500] Loss: 1.8166 TRA: 99.2232 TEA: 86.4721 TIME: 15.8400\n",
      "Train Epoch: [223/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.8579 TIME: 16.1100\n",
      "Train Epoch: [224/500] Loss: 1.8166 TRA: 99.3997 TEA: 87.6363 TIME: 15.5100\n",
      "Train Epoch: [225/500] Loss: 1.8175 TRA: 99.4350 TEA: 86.5951 TIME: 15.9600\n",
      "Train Epoch: [226/500] Loss: 1.8187 TRA: 99.2938 TEA: 81.6922 TIME: 15.7000\n",
      "Train Epoch: [227/500] Loss: 1.8166 TRA: 99.3644 TEA: 82.5777 TIME: 15.5600\n",
      "Train Epoch: [228/500] Loss: 1.8168 TRA: 99.4350 TEA: 85.6194 TIME: 15.3000\n",
      "Train Epoch: [229/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.1113 TIME: 15.2300\n",
      "Train Epoch: [230/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.8572 TIME: 15.2800\n",
      "Train Epoch: [231/500] Loss: 1.8166 TRA: 99.4350 TEA: 85.4308 TIME: 15.1200\n",
      "Train Epoch: [232/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.9720 TIME: 16.2000\n",
      "Train Epoch: [233/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.6279 TIME: 15.8200\n",
      "Train Epoch: [234/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.5459 TIME: 16.3100\n",
      "Train Epoch: [235/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.4721 TIME: 16.5600\n",
      "Train Epoch: [236/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4559 TIME: 16.0900\n",
      "Train Epoch: [237/500] Loss: 1.8165 TRA: 99.3997 TEA: 87.5051 TIME: 16.0700\n",
      "Train Epoch: [238/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.3575 TIME: 16.6200\n",
      "Train Epoch: [239/500] Loss: 1.8185 TRA: 99.4350 TEA: 86.0294 TIME: 15.3200\n",
      "Train Epoch: [240/500] Loss: 1.8167 TRA: 99.3644 TEA: 85.8818 TIME: 15.6600\n",
      "Train Epoch: [241/500] Loss: 1.8167 TRA: 99.3997 TEA: 85.9228 TIME: 15.2100\n",
      "Train Epoch: [242/500] Loss: 1.8166 TRA: 99.3997 TEA: 86.9804 TIME: 15.4200\n",
      "Train Epoch: [243/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.9476 TIME: 16.0300\n",
      "Train Epoch: [244/500] Loss: 1.8166 TRA: 99.3291 TEA: 84.2584 TIME: 15.2100\n",
      "Train Epoch: [245/500] Loss: 1.8177 TRA: 99.2585 TEA: 87.1034 TIME: 15.5500\n",
      "Train Epoch: [246/500] Loss: 1.8168 TRA: 99.2585 TEA: 86.7016 TIME: 15.3600\n",
      "Train Epoch: [247/500] Loss: 1.8166 TRA: 99.3997 TEA: 86.7754 TIME: 15.1700\n",
      "Train Epoch: [248/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.9720 TIME: 15.6200\n",
      "Train Epoch: [249/500] Loss: 1.8166 TRA: 99.2938 TEA: 78.3307 TIME: 15.8300\n",
      "Train Epoch: [250/500] Loss: 1.8168 TRA: 98.9054 TEA: 88.6038 TIME: 16.1800\n",
      "Train Epoch: [251/500] Loss: 1.8166 TRA: 99.3291 TEA: 86.8820 TIME: 16.1200\n",
      "Train Epoch: [252/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.5131 TIME: 15.5300\n",
      "Train Epoch: [253/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.3655 TIME: 15.7700\n",
      "Train Epoch: [254/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.6445 TIME: 16.2200\n",
      "Train Epoch: [255/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.3002 TIME: 16.4700\n",
      "Train Epoch: [256/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.3575 TIME: 16.0600\n",
      "Train Epoch: [257/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.4311 TIME: 15.7200\n",
      "Train Epoch: [258/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.8000 TIME: 16.8600\n",
      "Train Epoch: [259/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6279 TIME: 15.9200\n",
      "Train Epoch: [260/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0460 TIME: 16.4100\n",
      "Train Epoch: [261/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.2264 TIME: 16.7400\n",
      "Train Epoch: [262/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4231 TIME: 15.5700\n",
      "Train Epoch: [263/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9066 TIME: 16.6300\n",
      "Train Epoch: [264/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.0624 TIME: 16.7000\n",
      "Train Epoch: [265/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.0624 TIME: 15.7400\n",
      "Train Epoch: [266/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.7262 TIME: 16.9500\n",
      "Train Epoch: [267/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9558 TIME: 16.8000\n",
      "Train Epoch: [268/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6361 TIME: 16.2000\n",
      "Train Epoch: [269/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.6115 TIME: 16.0800\n",
      "Train Epoch: [270/500] Loss: 1.8166 TRA: 99.3997 TEA: 86.9148 TIME: 17.1100\n",
      "Train Epoch: [271/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.6358 TIME: 15.5300\n",
      "Train Epoch: [272/500] Loss: 1.8165 TRA: 99.3997 TEA: 87.5215 TIME: 16.2600\n",
      "Train Epoch: [273/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.5133 TIME: 16.2100\n",
      "Train Epoch: [274/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4477 TIME: 15.2400\n",
      "Train Epoch: [275/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.1444 TIME: 15.0300\n",
      "Train Epoch: [276/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.1690 TIME: 15.8400\n",
      "Train Epoch: [277/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8656 TIME: 15.5300\n",
      "Train Epoch: [278/500] Loss: 1.8166 TRA: 99.3997 TEA: 78.2078 TIME: 15.9400\n",
      "Train Epoch: [279/500] Loss: 1.8170 TRA: 99.3644 TEA: 85.8490 TIME: 16.2700\n",
      "Train Epoch: [280/500] Loss: 1.8166 TRA: 99.3997 TEA: 86.0130 TIME: 16.5200\n",
      "Train Epoch: [281/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.4636 TIME: 16.2000\n",
      "Train Epoch: [282/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.5215 TIME: 16.3000\n",
      "Train Epoch: [283/500] Loss: 1.8166 TRA: 99.4350 TEA: 88.1856 TIME: 16.2300\n",
      "Train Epoch: [284/500] Loss: 1.8165 TRA: 99.3997 TEA: 88.0134 TIME: 16.3200\n",
      "Train Epoch: [285/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.2015 TIME: 16.0400\n",
      "Train Epoch: [286/500] Loss: 1.8173 TRA: 99.3997 TEA: 87.8741 TIME: 15.8300\n",
      "Train Epoch: [287/500] Loss: 1.8165 TRA: 99.3997 TEA: 87.6363 TIME: 16.5400\n",
      "Train Epoch: [288/500] Loss: 1.8165 TRA: 99.3997 TEA: 87.7839 TIME: 16.3400\n",
      "Train Epoch: [289/500] Loss: 1.8165 TRA: 99.3997 TEA: 88.5382 TIME: 16.7000\n",
      "Train Epoch: [290/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.5464 TIME: 16.5600\n",
      "Train Epoch: [291/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.3906 TIME: 16.4800\n",
      "Train Epoch: [292/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.1692 TIME: 16.8700\n",
      "Train Epoch: [293/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.3660 TIME: 16.3100\n",
      "Train Epoch: [294/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.6527 TIME: 16.4600\n",
      "Train Epoch: [295/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.1610 TIME: 15.2500\n",
      "Train Epoch: [296/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.5874 TIME: 15.8100\n",
      "Train Epoch: [297/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.6855 TIME: 16.2300\n",
      "Train Epoch: [298/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.1610 TIME: 15.7300\n",
      "Train Epoch: [299/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.1198 TIME: 16.5900\n",
      "Train Epoch: [300/500] Loss: 1.8165 TRA: 99.3644 TEA: 85.8080 TIME: 15.4600\n",
      "Train Epoch: [301/500] Loss: 1.8167 TRA: 99.3291 TEA: 83.8485 TIME: 15.5600\n",
      "Train Epoch: [302/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.8082 TIME: 16.1100\n",
      "Train Epoch: [303/500] Loss: 1.8277 TRA: 99.3997 TEA: 84.4142 TIME: 16.2400\n",
      "Train Epoch: [304/500] Loss: 1.8165 TRA: 99.3644 TEA: 82.5039 TIME: 15.3900\n",
      "Train Epoch: [305/500] Loss: 1.8166 TRA: 99.3997 TEA: 84.9307 TIME: 15.5800\n",
      "Train Epoch: [306/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8328 TIME: 16.2300\n",
      "Train Epoch: [307/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8984 TIME: 15.7900\n",
      "Train Epoch: [308/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9148 TIME: 15.8300\n",
      "Train Epoch: [309/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6279 TIME: 15.9800\n",
      "Train Epoch: [310/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8656 TIME: 15.7800\n",
      "Train Epoch: [311/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9312 TIME: 15.8900\n",
      "Train Epoch: [312/500] Loss: 1.8166 TRA: 99.4350 TEA: 85.6194 TIME: 15.8500\n",
      "Train Epoch: [313/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.8408 TIME: 15.9200\n",
      "Train Epoch: [314/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.8326 TIME: 16.3100\n",
      "Train Epoch: [315/500] Loss: 1.8166 TRA: 99.4350 TEA: 85.7178 TIME: 16.0200\n",
      "Train Epoch: [316/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.2177 TIME: 15.6500\n",
      "Train Epoch: [317/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.4475 TIME: 16.4600\n",
      "Train Epoch: [318/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.9966 TIME: 16.4000\n",
      "Train Epoch: [319/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6934 TIME: 16.3500\n",
      "Train Epoch: [320/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.2015 TIME: 16.4500\n",
      "Train Epoch: [321/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8656 TIME: 17.0500\n",
      "Train Epoch: [322/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7508 TIME: 16.5800\n",
      "Train Epoch: [323/500] Loss: 1.8167 TRA: 99.4350 TEA: 85.3407 TIME: 16.4100\n",
      "Train Epoch: [324/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.2917 TIME: 16.3800\n",
      "Train Epoch: [325/500] Loss: 1.8165 TRA: 99.3997 TEA: 83.9551 TIME: 15.5800\n",
      "Train Epoch: [326/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6853 TIME: 16.3100\n",
      "Train Epoch: [327/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.1772 TIME: 15.1000\n",
      "Train Epoch: [328/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.2753 TIME: 16.6900\n",
      "Train Epoch: [329/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7262 TIME: 15.8200\n",
      "Train Epoch: [330/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.1687 TIME: 15.4300\n",
      "Train Epoch: [331/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.7098 TIME: 15.1400\n",
      "Train Epoch: [332/500] Loss: 1.8166 TRA: 99.3291 TEA: 86.6525 TIME: 15.9400\n",
      "Train Epoch: [333/500] Loss: 1.8165 TRA: 99.3997 TEA: 85.7014 TIME: 16.3600\n",
      "Train Epoch: [334/500] Loss: 1.8165 TRA: 99.3997 TEA: 85.1603 TIME: 15.1900\n",
      "Train Epoch: [335/500] Loss: 1.8166 TRA: 99.3644 TEA: 86.9640 TIME: 15.1100\n",
      "Train Epoch: [336/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.1116 TIME: 15.5400\n",
      "Train Epoch: [337/500] Loss: 1.8202 TRA: 99.3644 TEA: 83.4058 TIME: 15.5300\n",
      "Train Epoch: [338/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.9392 TIME: 15.3100\n",
      "Train Epoch: [339/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.7593 TIME: 15.2400\n",
      "Train Epoch: [340/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.0134 TIME: 15.3900\n",
      "Train Epoch: [341/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.6937 TIME: 15.3900\n",
      "Train Epoch: [342/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.5382 TIME: 14.9700\n",
      "Train Epoch: [343/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8328 TIME: 15.2200\n",
      "Train Epoch: [344/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.0048 TIME: 15.3600\n",
      "Train Epoch: [345/500] Loss: 1.8168 TRA: 99.4350 TEA: 86.9640 TIME: 15.5000\n",
      "Train Epoch: [346/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.4475 TIME: 16.0900\n",
      "Train Epoch: [347/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8082 TIME: 15.6700\n",
      "Train Epoch: [348/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.4885 TIME: 15.1600\n",
      "Train Epoch: [349/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9312 TIME: 15.2700\n",
      "Train Epoch: [350/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.2589 TIME: 15.3400\n",
      "Train Epoch: [351/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.7344 TIME: 15.2800\n",
      "Train Epoch: [352/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.1359 TIME: 15.0800\n",
      "Train Epoch: [353/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.9802 TIME: 15.1300\n",
      "Train Epoch: [354/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.3735 TIME: 15.3700\n",
      "Train Epoch: [355/500] Loss: 1.8165 TRA: 99.4350 TEA: 84.3650 TIME: 15.5300\n",
      "Train Epoch: [356/500] Loss: 1.8165 TRA: 99.4350 TEA: 84.9307 TIME: 15.3900\n",
      "Train Epoch: [357/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0214 TIME: 15.5400\n",
      "Train Epoch: [358/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9886 TIME: 15.2900\n",
      "Train Epoch: [359/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.4636 TIME: 15.1000\n",
      "Train Epoch: [360/500] Loss: 1.8176 TRA: 99.4350 TEA: 87.3330 TIME: 15.2800\n",
      "Train Epoch: [361/500] Loss: 1.8199 TRA: 99.4350 TEA: 86.4475 TIME: 15.2200\n",
      "Train Epoch: [362/500] Loss: 1.8165 TRA: 99.3997 TEA: 87.7183 TIME: 14.6000\n",
      "Train Epoch: [363/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.5461 TIME: 14.9900\n",
      "Train Epoch: [364/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4067 TIME: 15.3400\n",
      "Train Epoch: [365/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.8003 TIME: 15.2200\n",
      "Train Epoch: [366/500] Loss: 1.8167 TRA: 99.4350 TEA: 86.5213 TIME: 15.4400\n",
      "Train Epoch: [367/500] Loss: 1.8168 TRA: 99.4350 TEA: 86.9722 TIME: 15.4900\n",
      "Train Epoch: [368/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7508 TIME: 15.1300\n",
      "Train Epoch: [369/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0706 TIME: 15.8000\n",
      "Train Epoch: [370/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.3985 TIME: 15.0500\n",
      "Train Epoch: [371/500] Loss: 1.8166 TRA: 99.3997 TEA: 87.9807 TIME: 15.3300\n",
      "Train Epoch: [372/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.7839 TIME: 15.5200\n",
      "Train Epoch: [373/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.8495 TIME: 16.0800\n",
      "Train Epoch: [374/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.5710 TIME: 15.8000\n",
      "Train Epoch: [375/500] Loss: 1.8170 TRA: 99.4350 TEA: 86.2261 TIME: 16.0900\n",
      "Train Epoch: [376/500] Loss: 1.8166 TRA: 99.3291 TEA: 87.6363 TIME: 16.1300\n",
      "Train Epoch: [377/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.1851 TIME: 16.0400\n",
      "Train Epoch: [378/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8492 TIME: 15.5600\n",
      "Train Epoch: [379/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.6857 TIME: 16.5400\n",
      "Train Epoch: [380/500] Loss: 1.8165 TRA: 99.4350 TEA: 89.5630 TIME: 15.3400\n",
      "Train Epoch: [381/500] Loss: 1.8205 TRA: 99.4350 TEA: 88.8415 TIME: 15.3800\n",
      "Train Epoch: [382/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.5956 TIME: 15.2500\n",
      "Train Epoch: [383/500] Loss: 1.8165 TRA: 99.4350 TEA: 89.1203 TIME: 15.5000\n",
      "Train Epoch: [384/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.7267 TIME: 15.4500\n",
      "Train Epoch: [385/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.1938 TIME: 15.7000\n",
      "Train Epoch: [386/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.3332 TIME: 15.4100\n",
      "Train Epoch: [387/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.4398 TIME: 15.5000\n",
      "Train Epoch: [388/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.3824 TIME: 15.1300\n",
      "Train Epoch: [389/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.9973 TIME: 15.2600\n",
      "Train Epoch: [390/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.3742 TIME: 15.1200\n",
      "Train Epoch: [391/500] Loss: 1.8166 TRA: 99.3997 TEA: 86.9394 TIME: 15.0700\n",
      "Train Epoch: [392/500] Loss: 1.8165 TRA: 99.3644 TEA: 88.4070 TIME: 14.8000\n",
      "Train Epoch: [393/500] Loss: 1.8165 TRA: 99.3997 TEA: 87.5707 TIME: 15.0100\n",
      "Train Epoch: [394/500] Loss: 1.8166 TRA: 99.3997 TEA: 87.8249 TIME: 15.0200\n",
      "Train Epoch: [395/500] Loss: 1.8165 TRA: 99.3997 TEA: 87.9069 TIME: 15.3000\n",
      "Train Epoch: [396/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.2184 TIME: 15.1100\n",
      "Train Epoch: [397/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6934 TIME: 15.2100\n",
      "Train Epoch: [398/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.6281 TIME: 15.1700\n",
      "Train Epoch: [399/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.3330 TIME: 15.1300\n",
      "Train Epoch: [400/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.1936 TIME: 15.0200\n",
      "Train Epoch: [401/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6279 TIME: 15.2300\n",
      "Train Epoch: [402/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.6281 TIME: 15.1100\n",
      "Train Epoch: [403/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0378 TIME: 15.2800\n",
      "Train Epoch: [404/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.5953 TIME: 15.4100\n",
      "Train Epoch: [405/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0952 TIME: 15.1000\n",
      "Train Epoch: [406/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.7180 TIME: 15.3500\n",
      "Train Epoch: [407/500] Loss: 1.8321 TRA: 99.3644 TEA: 85.8736 TIME: 15.2400\n",
      "Train Epoch: [408/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.1528 TIME: 15.3000\n",
      "Train Epoch: [409/500] Loss: 1.8166 TRA: 99.4350 TEA: 88.4480 TIME: 15.3500\n",
      "Train Epoch: [410/500] Loss: 1.8197 TRA: 99.4350 TEA: 87.7019 TIME: 15.4500\n",
      "Train Epoch: [411/500] Loss: 1.8170 TRA: 99.3997 TEA: 86.1523 TIME: 15.0900\n",
      "Train Epoch: [412/500] Loss: 1.8165 TRA: 99.3644 TEA: 87.6773 TIME: 15.3700\n",
      "Train Epoch: [413/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4559 TIME: 15.2200\n",
      "Train Epoch: [414/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.4803 TIME: 15.3200\n",
      "Train Epoch: [415/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4641 TIME: 15.2100\n",
      "Train Epoch: [416/500] Loss: 1.8165 TRA: 99.3997 TEA: 87.5379 TIME: 16.0100\n",
      "Train Epoch: [417/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7180 TIME: 16.6500\n",
      "Train Epoch: [418/500] Loss: 1.8171 TRA: 99.3644 TEA: 87.8823 TIME: 15.6000\n",
      "Train Epoch: [419/500] Loss: 1.8178 TRA: 99.3997 TEA: 87.9725 TIME: 16.4000\n",
      "Train Epoch: [420/500] Loss: 1.8165 TRA: 99.3644 TEA: 88.0708 TIME: 16.1600\n",
      "Train Epoch: [421/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.5789 TIME: 16.6400\n",
      "Train Epoch: [422/500] Loss: 1.8165 TRA: 99.1879 TEA: 87.5543 TIME: 15.9600\n",
      "Train Epoch: [423/500] Loss: 1.8168 TRA: 99.3291 TEA: 88.0298 TIME: 15.8600\n",
      "Train Epoch: [424/500] Loss: 1.8165 TRA: 99.3644 TEA: 86.3491 TIME: 15.2900\n",
      "Train Epoch: [425/500] Loss: 1.8166 TRA: 99.3997 TEA: 86.4311 TIME: 15.4400\n",
      "Train Epoch: [426/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.8080 TIME: 16.3000\n",
      "Train Epoch: [427/500] Loss: 1.8166 TRA: 99.4350 TEA: 80.7494 TIME: 15.4900\n",
      "Train Epoch: [428/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.1116 TIME: 15.4000\n",
      "Train Epoch: [429/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.5543 TIME: 15.4400\n",
      "Train Epoch: [430/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4887 TIME: 15.4200\n",
      "Train Epoch: [431/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.0626 TIME: 15.7800\n",
      "Train Epoch: [432/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4067 TIME: 15.7400\n",
      "Train Epoch: [433/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.6773 TIME: 15.8700\n",
      "Train Epoch: [434/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.7839 TIME: 15.1900\n",
      "Train Epoch: [435/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.9151 TIME: 15.3600\n",
      "Train Epoch: [436/500] Loss: 1.8165 TRA: 99.4350 TEA: 88.4890 TIME: 14.9300\n",
      "Train Epoch: [437/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.8249 TIME: 15.1700\n",
      "Train Epoch: [438/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.7921 TIME: 14.9300\n",
      "Train Epoch: [439/500] Loss: 1.8168 TRA: 99.4350 TEA: 88.0626 TIME: 15.4300\n",
      "Train Epoch: [440/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.2018 TIME: 15.1500\n",
      "Train Epoch: [441/500] Loss: 1.8169 TRA: 99.4350 TEA: 87.3493 TIME: 15.0100\n",
      "Train Epoch: [442/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4231 TIME: 14.8900\n",
      "Train Epoch: [443/500] Loss: 1.8167 TRA: 99.4350 TEA: 88.2676 TIME: 14.9400\n",
      "Train Epoch: [444/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.5543 TIME: 15.0200\n",
      "Train Epoch: [445/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.7593 TIME: 14.9600\n",
      "Train Epoch: [446/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7672 TIME: 15.2300\n",
      "Train Epoch: [447/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9558 TIME: 14.9800\n",
      "Train Epoch: [448/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.8900 TIME: 15.1600\n",
      "Train Epoch: [449/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0050 TIME: 15.1400\n",
      "Train Epoch: [450/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.0378 TIME: 15.6700\n",
      "Train Epoch: [451/500] Loss: 1.8165 TRA: 99.3997 TEA: 87.3657 TIME: 15.3800\n",
      "Train Epoch: [452/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.3411 TIME: 14.9800\n",
      "Train Epoch: [453/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4231 TIME: 15.2000\n",
      "Train Epoch: [454/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.2018 TIME: 15.9000\n",
      "Train Epoch: [455/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.0788 TIME: 14.9200\n",
      "Train Epoch: [456/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.1444 TIME: 15.2000\n",
      "Train Epoch: [457/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8164 TIME: 16.1000\n",
      "Train Epoch: [458/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.1362 TIME: 15.9800\n",
      "Train Epoch: [459/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.5374 TIME: 15.2300\n",
      "Train Epoch: [460/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.7754 TIME: 15.7100\n",
      "Train Epoch: [461/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.2100 TIME: 15.9700\n",
      "Train Epoch: [462/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.1851 TIME: 15.6400\n",
      "Train Epoch: [463/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.5784 TIME: 15.8000\n",
      "Train Epoch: [464/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9148 TIME: 15.1500\n",
      "Train Epoch: [465/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.4390 TIME: 15.2500\n",
      "Train Epoch: [466/500] Loss: 1.8171 TRA: 99.4350 TEA: 86.9230 TIME: 15.0600\n",
      "Train Epoch: [467/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7344 TIME: 15.4500\n",
      "Train Epoch: [468/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7836 TIME: 15.2600\n",
      "Train Epoch: [469/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.3737 TIME: 15.7300\n",
      "Train Epoch: [470/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.4147 TIME: 15.6100\n",
      "Train Epoch: [471/500] Loss: 1.8165 TRA: 99.3997 TEA: 86.7508 TIME: 15.5000\n",
      "Train Epoch: [472/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.4393 TIME: 16.5700\n",
      "Train Epoch: [473/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6361 TIME: 16.3800\n",
      "Train Epoch: [474/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6689 TIME: 15.9800\n",
      "Train Epoch: [475/500] Loss: 1.8173 TRA: 99.4350 TEA: 86.3655 TIME: 16.1000\n",
      "Train Epoch: [476/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.5869 TIME: 16.3800\n",
      "Train Epoch: [477/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.4639 TIME: 17.1000\n",
      "Train Epoch: [478/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9066 TIME: 16.0100\n",
      "Train Epoch: [479/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7918 TIME: 16.5000\n",
      "Train Epoch: [480/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.6771 TIME: 16.3000\n",
      "Train Epoch: [481/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.3983 TIME: 15.8600\n",
      "Train Epoch: [482/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.3245 TIME: 16.5300\n",
      "Train Epoch: [483/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.9802 TIME: 16.2800\n",
      "Train Epoch: [484/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.5295 TIME: 15.1300\n",
      "Train Epoch: [485/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.3737 TIME: 15.0600\n",
      "Train Epoch: [486/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.8246 TIME: 15.1300\n",
      "Train Epoch: [487/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.0947 TIME: 15.3900\n",
      "Train Epoch: [488/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.0701 TIME: 15.0800\n",
      "Train Epoch: [489/500] Loss: 1.8165 TRA: 99.4350 TEA: 85.9474 TIME: 15.3400\n",
      "Train Epoch: [490/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.1523 TIME: 15.1100\n",
      "Train Epoch: [491/500] Loss: 1.8166 TRA: 99.4350 TEA: 86.2589 TIME: 14.9200\n",
      "Train Epoch: [492/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.5131 TIME: 15.5500\n",
      "Train Epoch: [493/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.2179 TIME: 15.3200\n",
      "Train Epoch: [494/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.9066 TIME: 15.4800\n",
      "Train Epoch: [495/500] Loss: 1.8166 TRA: 99.4350 TEA: 87.1772 TIME: 15.4200\n",
      "Train Epoch: [496/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7262 TIME: 15.1200\n",
      "Train Epoch: [497/500] Loss: 1.8165 TRA: 99.4350 TEA: 87.4149 TIME: 15.0400\n",
      "Train Epoch: [498/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.1031 TIME: 15.3400\n",
      "Train Epoch: [499/500] Loss: 1.8165 TRA: 99.4350 TEA: 86.7262 TIME: 15.4800\n"
     ]
    }
   ],
   "source": [
    "best_loss = 999\n",
    "best_acc = 0\n",
    "train_losses = []\n",
    "\n",
    "total_train_time = time.time()\n",
    "\n",
    "for epoch in range(epoch_start, args.epochs):\n",
    "    if args.backbone in args.SSISO:\n",
    "        train_loss, train_accuracy, test_accuracy, train_time\\\n",
    "                            = trainer.train_SSISO(epoch, model, super_head, criterion, train_loader, test_loader, optimizer, args)\n",
    "    elif args.backbone in args.SMIMO:\n",
    "        train_loss, train_accuracy, test_accuracy, train_time\\\n",
    "                            = trainer.train_SMIMO(epoch, model, criterion, train_loader, test_loader, optimizer, args)\n",
    "    elif args.backbone in args.SMISO:\n",
    "        train_loss, train_accuracy, test_accuracy, train_time \\\n",
    "                            = trainer.train_SMISO(epoch, model, criterion, train_loader, test_loader, optimizer, args)\n",
    "    elif args.backbone in args.MMISO:\n",
    "        train_loss, train_accuracy, test_accuracy, train_time \\\n",
    "                            = trainer.train_MMISO(epoch, model, criterion, train_loader, test_loader, optimizer, args)\n",
    "    elif args.backbone in args.MMIMO:\n",
    "        train_loss, train_accuracy, test_accuracy, train_time \\\n",
    "                            = trainer.train_MMIMO(epoch, model, criterion, train_loader, test_loader, optimizer, args)\n",
    "    else:\n",
    "        raise NotImplementedError(\"NO this model\")\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    with open(os.path.join(args.result_dir, \"log.csv\"), 'a+', encoding='gbk') as f:\n",
    "        row=[[\"epoch\", epoch, \n",
    "            \"loss\", train_loss, \n",
    "            \"train_accuracy\", round(train_accuracy, 2),\n",
    "            \"test_accuracy\", round(test_accuracy, 2),\n",
    "            \"train_time\", round(train_time, 2),\n",
    "            '\\n']]\n",
    "        write=csv.writer(f)\n",
    "        for i in range(len(row)):\n",
    "            write.writerow(row[i])\n",
    "\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict()}, \n",
    "                os.path.join(args.result_dir, \"model_loss.pth\"))\n",
    "\n",
    "    if best_acc < test_accuracy:\n",
    "        best_acc = test_accuracy\n",
    "        torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict()}, \n",
    "                os.path.join(args.result_dir, \"model_acc.pth\"))\n",
    "        \n",
    "total_train_time = time.time() - total_train_time\n",
    "\n",
    "torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()}, \n",
    "os.path.join(args.result_dir, \"model_last.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存最优结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from: /home/leo/Multimodal_Classification/MyMultiModal/result/03-25-17-51-DSHFNet/model_acc.pth\n",
      "Accuracy: 10924/12197 (89.56%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# args.result_dir = \"/home/leo/Multimodal_Classification/MyMultiModal/result/03-25-12-26-MIViT\"\n",
    "args.resume = os.path.join(args.result_dir, \"model_acc.pth\")\n",
    "# args.resume = os.path.join(args.result_dir, \"model_loss.pth\")\n",
    "if args.resume != '':\n",
    "    checkpoint = torch.load(args.resume)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    print('Loaded from: {}'.format(args.resume))\n",
    "else:\n",
    "    epoch_start = 0\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "if args.backbone in args.SSISO:\n",
    "    test_losses, test_preds, correct, targets = \\\n",
    "        tester.linear_test_SSISO(model, super_head, criterion, test_loader, args)\n",
    "if args.backbone in args.SMIMO:\n",
    "    test_losses, test_preds, correct, targets = \\\n",
    "        tester.linear_test_SMIMO(model, criterion, test_loader, args)\n",
    "elif args.backbone in args.SMISO:\n",
    "    test_losses, test_preds, correct, targets = \\\n",
    "        tester.linear_test_SMISO(model, criterion, test_loader, args)\n",
    "elif args.backbone in args.MMISO:\n",
    "    test_losses, test_preds, correct, targets = \\\n",
    "        tester.linear_test_MMISO(model, criterion, test_loader, args)\n",
    "elif args.backbone in args.MMIMO:\n",
    "    test_losses, test_preds, correct, targets = \\\n",
    "        tester.linear_test_MMIMO(model, criterion, test_loader, args)\n",
    "classification, kappa = tester.get_results(test_preds, targets)\n",
    "\n",
    "test_time = time.time() - tic\n",
    "\n",
    "with open(os.path.join(args.result_dir, \"log_final.csv\"), 'a+', encoding='gbk') as f:\n",
    "    row=[[\"training\",\n",
    "        \"\\nepoch\", epoch, \n",
    "        \"\\ndata_name = \" + str(args.dataset_name),\n",
    "        \"\\nbatch_size = \" + str(args.batch_size),\n",
    "        \"\\npatch_size = \" + str(args.patch_size),\n",
    "        \"\\nnum_components = \" + str(args.components),\n",
    "        '\\n' + classification,\n",
    "        \"\\nkappa = \\t\\t\\t\" + str(round(kappa, 4)),\n",
    "        \"\\ntotal_time = \", round(total_train_time, 2),\n",
    "        '\\ntest time = \\t' + str(round(test_time, 2)),\n",
    "        ]]\n",
    "    write=csv.writer(f)\n",
    "    for i in range(len(row)):\n",
    "        write.writerow(row[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.resume = os.path.join(args.result_dir, \"joint_model_oa.pth\")\n",
    "# if args.resume != '':\n",
    "#     checkpoint = torch.load(args.resume)\n",
    "#     net.load_state_dict(checkpoint['base'], strict=False)\n",
    "#     epoch_start = checkpoint['epoch'] + 1\n",
    "#     print('Loaded from: {}'.format(args.resume))\n",
    "# else:\n",
    "#     epoch_start = 0\n",
    "\n",
    "# _, groundTruth = data_reader.load_data(args.dataset_name, path_data=args.path_data, type_data=\"Houston\")\n",
    "\n",
    "# args.remove_zero_labels = False\n",
    "# args.train_ratio = 1\n",
    "# img1, img2, train_gt, test_gt, data_gt = data_pipe_D.get_data(args)\n",
    "\n",
    "# # create dataloader\n",
    "# data_dataset = HyperX(img1, img2, data_gt, transform=None, \n",
    "#                        patch_size=args.patch_size, \n",
    "#                        flip_augmentation=False, \n",
    "#                         radiation_augmentation=False, mixture_augmentation=False, \n",
    "#                         remove_zero_labels=args.remove_zero_labels)\n",
    "# data_loader = torch.utils.data.DataLoader(data_dataset, batch_size=args.batch_size, \\\n",
    "#                                            shuffle=False, drop_last=False)\n",
    "\n",
    "# KNNCA, KNNOA, KNNAA, KNNKA = KNN.test(net, memory_loader, data_loader, class_num, \\\n",
    "#                                       groundTruth, args, visulation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画 loss 曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # args.plot_loss_curve = True\n",
    "# if args.plot_loss_curve:\n",
    "#     fig = plt.figure()\n",
    "#     plt.plot(range(args.epochs), train_losses, color='blue')\n",
    "#     plt.plot(range(args.epochs), loss_contras, color='red')\n",
    "#     plt.plot(range(args.epochs), loss_supers, color='pink')\n",
    "\n",
    "#     # test_counter 是先定义好的，test——losses 训练一轮记录一次\n",
    "#     # plt.scatter(test_counter, test_losses, color='red')\n",
    "#     plt.legend(['train_losses', 'loss_contras', 'loss_supers'], loc='upper right')\n",
    "#     plt.xlabel('number of training examples seen')\n",
    "#     plt.ylabel('negative log likelihood loss')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for idx_o, (data, _, target) in enumerate(test_loader):\n",
    "#         target = target - 1\n",
    "#         data = data.to(args.device)\n",
    "        \n",
    "#         output = super_head(net(data))\n",
    "#         target = target.to(args.device)\n",
    "\n",
    "#         for idx, _ in enumerate(output.cpu().numpy()):\n",
    "#             # print(output.cpu().numpy().shape)\n",
    "#             if idx == 0 and idx_o == 0:\n",
    "#                 list_output = output.cpu().numpy()[idx]\n",
    "#                 list_target = target.cpu().numpy()[idx]\n",
    "#             # print(list_output.shape, list_target.shape)\n",
    "\n",
    "#             if idx < 100:\n",
    "#                 list_output = np.vstack((list_output, output.cpu().numpy()[idx]))\n",
    "#                 list_target = np.append(list_target, target.cpu().numpy()[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE()\n",
    "\n",
    "# out = tsne.fit_transform(list_output)\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_axis_off()\n",
    "# ax.xaxis.set_visible(False)\n",
    "# ax.yaxis.set_visible(False)\n",
    "# # fig.set_size_inches(label.shape[1] * scale / dpi, label.shape[0] * scale / dpi)\n",
    "# plt.gca().xaxis.set_major_locator(plt.NullLocator())\n",
    "# plt.gca().yaxis.set_major_locator(plt.NullLocator())\n",
    "# plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)\n",
    "\n",
    "# label = [\"Broccoli-weeds-1\",\"Broccoli-weeds-2\",\"Fallow\",\"Fallow-rough-plow\",\n",
    "#          \"Fallow-smooth\",\"Stubble\",\"Celery\",\"Grapes-untrained\",\n",
    "#          \"Soil-senesced-develop\",\"Corn-weeds\",\"Lettuce-4wk\",\"Lettuce-5wk\",\n",
    "#          \"Lettuce-6wk\",\"Lettuce-7wk\",\"Vinyard-untrained\",\n",
    "#          \"Vinyard-vertical-trellis\"]\n",
    "# for i in range(class_num):\n",
    "#     indices = list_target  == i\n",
    "#     x, y = out[indices].T\n",
    "#     # plt.scatter(x, y, s=5, label=label[i])\n",
    "#     plt.scatter(x, y, s=5, label=str(i+1))\n",
    "# plt.legend(loc=2,bbox_to_anchor=(1.05,1.0),borderaxespad = 0., fontsize=12)\n",
    "# plt.savefig(args.result_dir  + '/tsneFull' + '.png', dpi=400, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
